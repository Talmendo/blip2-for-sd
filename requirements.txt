--extra-index-url https://download.pytorch.org/whl/cu118
accelerate
transformers
torch==2.0.1+cu118
tqdm
Pillow
scipy
# bitsandbytes # use bitsandytes for 4bit quantization, set use_4bit=True in load_model. Probably won't work on windows. Note that your local cuda version must be the same as the version torch was compiled with.